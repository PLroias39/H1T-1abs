# 实验一：从零开始实现基于Transformer的语言模型（2025年）
## 一、实验内容
Transformer模型是一种通用的深度学习架构，已经在各种自然语言处理和其他领域的任务中取得了巨大成功。其中Decoder-only的Transformer模型（如GPT系列）在许多领域中发挥着重要作用。这种模型主要集中于生成任务。
在本实验中，请你基于实验提供的代码框架，实现一个Decoder-only的Transformer模型——GPT，并使用SkyPile中文预训练数据集的一个子集进行模型训练。
暂时无法在飞书文档外展示此内容
## 二、实验要求
实验总分为20分，包括以下内容：
1. 基于提供的代码框架，实现GPT模型
  1. 实现Token Embedding（0.5分）
  2. 实现Positional Encoding（0.5分）
  3. 实现简易Attention Mask（2分）
  4. 实现多头自注意力（5分）
  5. 实现FFN层（2分）
  6. 实现Decoder Block（2分）
  7. 实现完整GPT（5分）
2. 使用SkyPile数据集和提供的训练框架，训练GPT模型，观察训练日志。（0.5分）
3. 加载训练后的模型，测试其生成能力。（0.5分）
4. （可选）增大模型规模、调整超参数，使模型拥有更好的生成能力
5. 撰写实验报告，简要描述设计思路、设计逻辑、遇到的困难和解决方式等，提供程序运行效果的截图。（2分）
## 三、提交方式
- 请将代码（仅ipynb文件）、模型检查点、实验报告（参照实验报告模板）打包提交至链接（压缩包注明学号-姓名）
<br><br><br>

非常简单，代码框架和注释已给出，只需填空即可
