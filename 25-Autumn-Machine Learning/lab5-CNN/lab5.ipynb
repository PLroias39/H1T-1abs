{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPpG40mTPCjG67TkKVb1lSn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["实验五：卷积神经网络实验<br>\n","实验内容：采用任意一种课程中介绍过的或者其它卷积神经网络模型（如Lenet-5、AlexNet等）用于解决某种媒体类型的模式识别问题。<br>\n","要求：<br>\n","卷积神经网络可以基于现有框架如TensorFlow、Pytorch或者Mindspore等构建，也可以自行设计实现。<br>\n","数据集可以使用手写体数字图像标准数据集，也可以自行构建。预测问题可以包括分类或者回归等。\n","实验工作还需要对激活函数的选择、dropout等技巧的使用做实验分析。必要时上网查找有关参考文献。<br>\n","用不同数据量，不同超参数，比较实验效果，并给出截图和分析"],"metadata":{"id":"CPs_LFC1yQOS"}},{"cell_type":"markdown","source":["# 环境配置"],"metadata":{"id":"mxH-fJJlzUrU"}},{"cell_type":"code","source":["!nvidia-smi  # 查看GPU是否可用（可选）\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, models\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from typing import Dict\n","import time\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)"],"metadata":{"id":"qOMdDOBAyV2E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 模型定义"],"metadata":{"id":"1VZjuIAohiyh"}},{"cell_type":"markdown","source":["## LeNet5"],"metadata":{"id":"7iSuUzRYzWg6"}},{"cell_type":"code","source":["class LeNet5(nn.Module):\n","    def __init__(self, num_classes=10, in_channels=1,\n","           activation='relu', dropout=0.0, use_bn=False):\n","\n","        super(LeNet5, self).__init__()\n","\n","        self.num_classes = num_classes\n","        self.dropout_p = dropout\n","        self.use_bn = use_bn\n","        # 激活函数映射\n","        self.activation = self._get_activation(activation)\n","\n","        self.conv1 = nn.Conv2d(in_channels, 6, kernel_size=5, stride=1, padding=2)\n","        self.pool1 = nn.AvgPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n","        self.pool2 = nn.AvgPool2d(2, 2)\n","\n","        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, num_classes)\n","\n","    def _get_activation(self, name):\n","        act_map = {\n","            \"relu\": nn.ReLU(),\n","            \"leakyrelu\": nn.LeakyReLU(0.1),\n","            \"elu\": nn.ELU(),\n","            \"selu\": nn.SELU(),\n","            \"gelu\": nn.GELU()\n","        }\n","        return act_map.get(name.lower(), nn.ReLU())\n","\n","    def forward(self, x):\n","        x = self.activation(self.conv1(x))\n","        x = self.pool1(x)\n","        x = self.activation(self.conv2(x))\n","        x = self.pool2(x)\n","\n","        x = x.view(x.size(0), -1)\n","        x = F.dropout(x, p=self.dropout_p, training=self.training)\n","        x = self.activation(self.fc1(x))\n","        x = self.activation(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n"],"metadata":{"id":"P9o0vn6WzY9m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## VGG11"],"metadata":{"id":"0XpDDIb01OgF"}},{"cell_type":"code","source":["class VGG11(nn.Module):\n","    def __init__(self, num_classes=10, in_channels=3,\n","           activation='relu', dropout=0.0, use_bn=False):\n","\n","        super(VGG11, self).__init__()\n","        self.conv_layer1 = self._make_conv_1(in_channels,64)\n","        self.conv_layer2 = self._make_conv_1(64,128)\n","        self.conv_layer3 = self._make_conv_2(128,256)\n","        self.conv_layer4 = self._make_conv_2(256,512)\n","        self.conv_layer5 = self._make_conv_2(512,512)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512, 64),    # 此处修改输入输出维度\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=0.5),\n","            nn.Linear(64, 64),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=0.5),\n","            nn.Linear(64, num_classes)\n","        )\n","    def _make_conv_1(self,in_channels,out_channels):\n","        layer = nn.Sequential(\n","                nn.Conv2d(in_channels,out_channels, kernel_size=3, padding=1),\n","                # batchnorm\n","                nn.BatchNorm2d(out_channels, affine=True),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(kernel_size=2, stride=2)\n","            )\n","        return layer\n","    def _make_conv_2(self,in_channels,out_channels):\n","        layer = nn.Sequential(\n","                nn.Conv2d(in_channels,out_channels, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(out_channels, affine=True),\n","                nn.ReLU(inplace=True),\n","\n","                nn.Conv2d(out_channels,out_channels, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(out_channels, affine=True),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(kernel_size=2, stride=2)\n","              )\n","        return layer\n","\n","    def forward(self, x):\n","        # 32*32 channel == 3\n","        x = self.conv_layer1(x)\n","        # 16*16 channel == 64\n","        x = self.conv_layer2(x)\n","        # 8*8 channel == 128\n","        x = self.conv_layer3(x)\n","        # 4*4 channel == 256\n","        x = self.conv_layer4(x)\n","        # 2*2 channel == 512\n","        x = self.conv_layer5(x)\n","        # 1*1 channel == 512\n","        x = x.view(x.size(0), -1)\n","        # 512\n","        x = self.classifier(x)\n","        # 10\n","        return x"],"metadata":{"id":"VtInUSrt1Qgm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## mobilenetv2"],"metadata":{"id":"r1FUqy0-hNAK"}},{"cell_type":"code","source":["def get_mobilenetv2(num_classes=10, pretrained=False):\n","    model = models.mobilenet_v2(pretrained=pretrained)\n","    model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n","    return model\n","\n","\n","class MobileNetV2(nn.Module):\n","    def __init__(self, num_classes=10, ispretrained=False):\n","        super().__init__()\n","        self.base = models.mobilenet_v2(pretrained=ispretrained)\n","        self.base.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n","        self.base.classifier[1] = nn.Linear(self.base.last_channel, num_classes)\n","    def forward(self, x):\n","        return self.base(x)\n"],"metadata":{"id":"sInJK_yp1wrs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 数据加载和增强"],"metadata":{"id":"Q1xfYV6jCNgw"}},{"cell_type":"code","source":["def getData(dataset_name = \"MNIST\", batch_size = 128, aug_level=\"none\"):\n","  # 设定不同增强强度\n","  if aug_level == \"none\":\n","      transform_train = transforms.Compose([\n","          transforms.Resize((32, 32)),\n","          transforms.ToTensor(),\n","      ])\n","  elif aug_level == \"basic\":\n","      transform_train = transforms.Compose([\n","          transforms.Resize((32, 32)),\n","          transforms.RandomHorizontalFlip(),\n","          transforms.RandomCrop(32, padding=4),\n","          transforms.ColorJitter(brightness=0.2, contrast=0.2),\n","          transforms.ToTensor(),\n","      ])\n","  elif aug_level == \"strong\":\n","      transform_train = transforms.Compose([\n","          transforms.Resize((32, 32)),\n","          transforms.RandomHorizontalFlip(),\n","          transforms.RandomRotation(30),\n","          transforms.RandomCrop(32, padding=4),\n","          transforms.ToTensor(),\n","          transforms.RandomErasing(p=0.3),\n","      ])\n","\n","  transform_test = transforms.Compose([\n","      transforms.Resize((32, 32)),\n","      transforms.ToTensor(),\n","  ])\n","  dataset_name = dataset_name.upper()\n","  # 选择数据集\n","  if dataset_name == \"MNIST\":\n","      transform_train = transforms.Compose([\n","          transform_train,\n","          transforms.Normalize((0.5,), (0.5,)),\n","      ])\n","      transform_test = transforms.Compose([\n","          transform_test,\n","          transforms.Normalize((0.5,), (0.5,)),\n","      ])\n","      train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform_train)\n","      test_dataset  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform_test)\n","      in_channels = 1\n","\n","  elif dataset_name == \"CIFAR10\":\n","      transform_train = transforms.Compose([\n","          transform_train,\n","          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","      ])\n","      transform_test = transforms.Compose([\n","          transform_test,\n","          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","      ])\n","      train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n","      test_dataset  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n","      in_channels = 3\n","\n","  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","  test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","  return train_loader, test_loader, in_channels\n","\n","  print(f\"{dataset_name} loaded: {len(train_dataset)} training samples, {len(test_dataset)} test samples, aug_level = {aug_level}\")\n"],"metadata":{"id":"lm_4JYQLyh_Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 模型加载"],"metadata":{"id":"uLkB_Ixsi6m4"}},{"cell_type":"code","source":["def get_model(name, num_classes=10, in_channels=3, ispretrained=False, activation='relu', dropout=0.0, use_bn=False):\n","    name = name.lower()\n","    if name == \"lenet5\":\n","        return LeNet5(num_classes=num_classes, in_channels=in_channels, activation=activation, dropout=dropout, use_bn=use_bn)\n","    elif name == \"vgg11\":\n","        return VGG11(num_classes=num_classes, in_channels=in_channels, activation=activation, dropout=dropout, use_bn=use_bn)\n","    # 暂不考虑resnet18\n","    elif name == \"resnet18\":\n","        model = models.resnet18(pretrained=ispretrained)\n","        model.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","        return model\n","    elif name == \"mobilenetv2\":\n","        model = models.mobilenet_v2(pretrained=ispretrained)\n","        model.features[0][0] = nn.Conv2d(in_channels, 32, kernel_size=3, stride=2, padding=1, bias=False)\n","        model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n","        return model\n","    else:\n","        raise ValueError(f\"Unknown model name: {name}\")"],"metadata":{"id":"Hhoy7GCMEkfO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 核心训练流程"],"metadata":{"id":"u0XY2eJdic7f"}},{"cell_type":"code","source":["def train_one_epoch(model, device, loader, optimizer, criterion):\n","    model.train()\n","    total_loss, correct, total = 0.0, 0, 0\n","    for x, y in loader:\n","        x, y = x.to(device), y.to(device)\n","        optimizer.zero_grad()\n","        out = model(x)\n","        loss = criterion(out, y)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        _, pred = out.max(1)\n","        total += y.size(0)\n","        correct += pred.eq(y).sum().item()\n","    return total_loss / len(loader), 100. * correct / total\n","\n","\n","def test_one_epoch(model, device, loader, criterion):\n","    model.eval()\n","    total_loss, correct, total = 0.0, 0, 0\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x, y = x.to(device), y.to(device)\n","            out = model(x)\n","            loss = criterion(out, y)\n","            total_loss += loss.item()\n","            _, pred = out.max(1)\n","            total += y.size(0)\n","            correct += pred.eq(y).sum().item()\n","    return total_loss / len(loader), 100. * correct / total\n","\n","def run_training(model, train_loader, test_loader, optimizer, criterion, num_epochs=10):\n","    start_time = time.time()\n","    train_losses, test_losses, train_accs, test_accs = [], [], [], []\n","\n","    for epoch in range(num_epochs):\n","        tr_loss, tr_acc = train_one_epoch(model, device, train_loader, optimizer, criterion)\n","        te_loss, te_acc = test_one_epoch(model, device, test_loader, criterion)\n","\n","        train_losses.append(tr_loss)\n","        test_losses.append(te_loss)\n","        train_accs.append(tr_acc)\n","        test_accs.append(te_acc)\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}: TrainLoss={tr_loss:.4f}, TestAcc={te_acc:.2f}%\")\n","\n","    elapsed = (time.time() - start_time) / num_epochs\n","    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","    return {\n","        \"train_losses\": train_losses,\n","        \"test_losses\": test_losses,\n","        \"train_accs\": train_accs,\n","        \"test_accs\": test_accs,\n","        \"param_count\": params,\n","        \"epoch_time\": elapsed,\n","        \"final_acc\": test_accs[-1]\n","    }\n"],"metadata":{"id":"v7kwuGqF57Pr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 批量运行实验"],"metadata":{"id":"F4fvWpjwFbq9"}},{"cell_type":"code","source":["def run_experiment(config):\n","    print(f\"\\n=== Running {config['model']} | {config['dataset']} | {config['activation']} ===\")\n","\n","    # dataset config: augment\n","    train_loader, test_loader, in_ch = getData(config[\"dataset\"], batch_size=128, aug_level=config[\"augment\"])\n","\n","    # model config: activation, dropout, batchnorm\n","    model = get_model(name=config['model'], num_classes=10, in_channels=in_ch,\n","                   activation=config[\"activation\"],\n","                   dropout=config[\"dropout\"],\n","                   use_bn=config[\"use_bn\"]).to(device)\n","\n","    # training config: optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer_type = config[\"optimizer\"]\n","    if optimizer_type == \"SGD\":\n","        optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"], weight_decay=config.get(\"l2\", 0))\n","    elif optimizer_type == \"RMSprop\":\n","        optimizer = optim.RMSprop(model.parameters(), lr=config[\"lr\"], weight_decay=config.get(\"l2\", 0))\n","    else:\n","        optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config.get(\"l2\", 0))\n","\n","    result = run_training(model, train_loader, test_loader, optimizer, criterion, num_epochs=config[\"epochs\"])\n","    return result"],"metadata":{"id":"xC0wP16NE9iN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 主执行代码"],"metadata":{"id":"vl0OZCBqtr7j"}},{"cell_type":"code","source":["experiment1 = [\n","    {\"model\": \"lenet5\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"MNIST\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","\n","    {\"model\": \"vgg11\",\"activation\": \"relu\",\n","     \"dataset\": \"MNIST\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","\n","    {\"model\": \"lenet5\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","\n","    {\"model\": \"vgg11\",\"activation\": \"relu\",\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","\n","]\n","# 激活函数对比 LeNet5和CIFAR10\n","experiment2 = [\n","    {\"model\": \"vgg11\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","    {\"model\": \"vgg11\",\"activation\": \"leakyrelu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","    {\"model\": \"vgg11\",\"activation\": \"elu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","    {\"model\": \"vgg11\",\"activation\": \"selu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","    {\"model\": \"vgg11\",\"activation\": \"gelu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","]\n","# 正则化对比 LeNet5和CIFAR10\n","experiment3 = [\n","    {\"model\": \"vgg11\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","    {\"model\": \"vgg11\",\"activation\": \"relu\", \"dropout\": 0.3,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","    {\"model\": \"vgg11\",\"activation\": \"relu\", \"dropout\": 0.5,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","    {\"model\": \"vgg11\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": True,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","    {\"model\": \"vgg11\",\"activation\": \"relu\", \"dropout\": 0.3,\"use_bn\": True,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","    {\"model\": \"vgg11\",\"activation\": \"relu\", \"dropout\": 0.5,\"use_bn\": True,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","]\n","# 优化器对比 CIFAR10\n","experiment4 = [\n","    {\"model\": \"lenet5\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","    {\"model\": \"lenet5\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"SGD\", \"lr\": 0.001, \"epochs\": 5},\n","    {\"model\": \"lenet5\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"RMSprop\", \"lr\": 0.001, \"epochs\": 5},\n","\n","    {\"model\": \"vgg11\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","    {\"model\": \"vgg11\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"SGD\", \"lr\": 0.001, \"epochs\": 5},\n","    {\"model\": \"vgg11\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"RMSprop\", \"lr\": 0.001, \"epochs\": 5},\n","]\n","# 学习率对比 CIFAR10\n","experiment5 = [\n","    {\"model\": \"lenet5\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.003, \"epochs\": 5},\n","    {\"model\": \"lenet5\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.005, \"epochs\": 5},\n","    {\"model\": \"lenet5\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.008, \"epochs\": 5},\n","    {\"model\": \"lenet5\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.010, \"epochs\": 5},\n","\n","\n","\n","    {\"model\": \"vgg11\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.00003, \"epochs\": 5},\n","    {\"model\": \"vgg11\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.00005, \"epochs\": 5},\n","    {\"model\": \"vgg11\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.0001, \"epochs\": 5},\n","]\n","\n","# 数据增强对比\n","experiment6 = [\n","    {\"model\": \"lenet5\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"strong\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","    {\"model\": \"lenet5\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"basic\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","    {\"model\": \"lenet5\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","\n","    {\"model\": \"vgg11\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"strong\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","    {\"model\": \"vgg11\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"basic\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","    {\"model\": \"vgg11\",\"activation\": \"relu\", \"dropout\": 0.0,\"use_bn\": False,\n","     \"dataset\": \"CIFAR10\", \"augment\": \"none\",\n","     \"optimizer\": \"Adam\", \"lr\": 0.001, \"epochs\": 5},\n","]\n","\n","results = {}\n","for i, cfg in enumerate(experiment6):\n","    results[f\"exp{i+1}\"] = run_experiment(cfg)\n","\n"],"metadata":{"id":"P_orGYc1jOVs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for key, res in results.items():\n","    plt.figure(figsize=(10,4))\n","    plt.plot(res[\"train_losses\"], label=\"Train Loss\")\n","    plt.plot(res[\"test_losses\"], label=\"Test Loss\")\n","    plt.title(f\"{key} - Loss Curve\")\n","    plt.legend()\n","    plt.show()\n","\n","print(\"\\n=== 实验总结 ===\")\n","for k, r in results.items():\n","    print(f\"{k:6s} | Acc={r['final_acc']:.2f}% | Params={r['param_count']/1e3:.1f}K | EpochTime={r['epoch_time']:.2f}s\")"],"metadata":{"id":"1aq2tJI5jQvy"},"execution_count":null,"outputs":[]}]}